# NLP-BERT-Q-A
With The Stanford Question Answering Dataset     
https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html 
    
1) The model can be trained in a CPU machine.    
2) The size of the trained models are huge, so dint push to github repo.    
3) In a 32GB RAM windows machine, the model training took approx 2 hrs for 2 epochs for 5000rows of training data, ( to download pretrained model and train on our dataset and store in our local machine). 
